{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conection to the Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.4' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/DELL/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from prometheus_api_client import PrometheusConnect\n",
    "from datetime import datetime, timedelta  \n",
    "import pandas as pd \n",
    "import yaml\n",
    "import json\n",
    "prom = PrometheusConnect(url =\"http://192.168.56.10:30090\", disable_ssl=True)\n",
    "prom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch All Metric Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all metric names\n",
    "all_metrics = prom.all_metrics()\n",
    "print(f'{len(all_metrics)} metrics found')\n",
    "\n",
    "# Define the file path where you want to save the metrics\n",
    "file_path = 'prometheus_metrics.txt'\n",
    "\n",
    "# Write all metrics to the text file, one per line\n",
    "with open(file_path, 'w') as file:\n",
    "    for metric in all_metrics:\n",
    "        file.write(metric + '\\n')\n",
    "\n",
    "print(f\"Metrics have been saved to {file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query a Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a starting point for data collection.\n",
    "start_time = datetime.now() - timedelta(days=1)\n",
    "\n",
    "# Current time for the end of data collection\n",
    "end_time = datetime.now()\n",
    "\n",
    "# Step size for queries (e.g., \"15m\" for 15 minutes)\n",
    "step = '1m'\n",
    "\n",
    "# Define the metric you want to query\n",
    "metric = 'node_cpu_seconds_total'\n",
    "\n",
    "# Perform a range query for the metric\n",
    "result_range = prom.custom_query_range(\n",
    "    query=metric,\n",
    "    start_time=start_time,\n",
    "    end_time=end_time,\n",
    "    step=step\n",
    "    )\n",
    "\n",
    "print(json.dumps(result_range, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup Whole Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import os\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Setup connection\n",
    "prom = PrometheusConnect(url =\"http://192.168.56.10:30090\", disable_ssl=True)\n",
    "\n",
    "# Fetch all metric names\n",
    "all_metrics = prom.all_metrics()\n",
    "\n",
    "# Define a starting point for data collection\n",
    "start_time = datetime.now() - timedelta(days=1)  # Example: 5 years ago\n",
    "end_time = datetime.now()  # Current time\n",
    "step = '2m'  # Step size\n",
    "\n",
    "# if there is no path and dir is not exist will create it recursively\n",
    "path = f\"../data_json/{datetime.now().strftime('%Y%m%d')}/\"\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "for metric in all_metrics:\n",
    "    # Perform a range query for each metric\n",
    "    result_range = prom.custom_query_range(\n",
    "        query=metric,\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        step=step\n",
    "    )\n",
    "\n",
    "    # Define file path\n",
    "    file_path = path+f'{metric.replace(\"/\", \"_\")}.json'  # Replace '/' with '_' to avoid path issues\n",
    "\n",
    "    # Write data to a gzipped JSON file\n",
    "    with open(file_path, 'wt', encoding='utf-8') as file:\n",
    "        json.dump(result_range, file, indent=4)\n",
    "\n",
    "    print(f\"Data for {metric} has been saved to {file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import importlib as imp\n",
    "import helper as hp\n",
    "imp.reload(hp)\n",
    "\n",
    "cpu = hp.filter_filenames(path='../data_json/20240420/json/',\n",
    "                    substrings=['cpu', 'pod'], \n",
    "                    and_or='and')\n",
    "\n",
    "cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import json\n",
    "import helper as hp\n",
    "# Read JSON data from a gzipped file\n",
    "file_path = '../data_json/20240420/json/node_cpu_seconds_total.json'\n",
    "hp.read_json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import helper as hp\n",
    "from pandas import json_normalize\n",
    "import importlib as imp\n",
    "imp.reload(hp)\n",
    "# list of the file in following directory\n",
    "path = '../data_json/20240420/json/'\n",
    "files = hp.filter_filenames(path='../data_json/20240420/json/',\n",
    "                            substrings=['cpu'], \n",
    "                            and_or='and')\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "# node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate.json\n",
    "\n",
    "# Capturing the output\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "# Redirect stdout to capture print statements\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = mystdout = StringIO()\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "print('\\n\\n')\n",
    "\n",
    "for f in files:\n",
    "    print(str(f))\n",
    "    hp.read_json(path+f)\n",
    "\n",
    "# Restore stdout to original\n",
    "sys.stdout = old_stdout\n",
    "\n",
    "# Get the captured output\n",
    "output = mystdout.getvalue()\n",
    "\n",
    "# Writing the output to a text file\n",
    "with open('output.txt', 'w') as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate.json\"\n",
    "\"container_cpu_usage_seconds_total.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import helper as hp\n",
    "import importlib as imp\n",
    "imp.reload(hp)\n",
    "\n",
    "path = '../data_json/20240420/json/'\n",
    "\n",
    "f1 = \"cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests.json\"\n",
    "f2 = \"node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate.json\"\n",
    "f3 = \"container_cpu_usage_seconds_total.json\"\n",
    "\n",
    "dfs_memory = hp.transform_data(hp.read_json(path+f2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "from datetime import datetime, timedelta  \n",
    "import pandas as pd \n",
    "import yaml\n",
    "import json\n",
    "\n",
    "# Initialize Prometheus Connection\n",
    "prom = PrometheusConnect(url =\"http://192.168.56.10:30090\", disable_ssl=True)\n",
    "\n",
    "\n",
    "def get_total_cpu_usage_for_namespace(metric_name, namespaces, aggregation_time, start_time=None, end_time=None, step='60s', per_pod=False):\n",
    "\n",
    "    # Initialize the result dictionary\n",
    "    results = {}\n",
    "\n",
    "    # Loop through each namespace and fetch metrics\n",
    "    for namespace in namespaces:\n",
    "        label_part = f'namespace=\"{namespace}\"'\n",
    "        \n",
    "        # Modify the PromQL query to sum across all pods in the namespace\n",
    "        \n",
    "        if per_pod:\n",
    "            query = f'sum(rate({metric_name}{{{label_part}}}[{aggregation_time}])) by (pod)'\n",
    "        else:\n",
    "            query = f'sum(rate({metric_name}{{{label_part}}}[{aggregation_time}]))'\n",
    "\n",
    "\n",
    "        # Use default times if not specified\n",
    "        if not start_time:\n",
    "            start_time = datetime.datetime.now() - datetime.timedelta(hours=1)\n",
    "        if not end_time:\n",
    "            end_time = datetime.datetime.now()\n",
    "\n",
    "        # Convert datetime objects to ISO format\n",
    "        # start_time_iso = start_time.isoformat() + 'Z'\n",
    "        # end_time_iso = end_time.isoformat() + 'Z'\n",
    "\n",
    "        # Fetch metrics from Prometheus\n",
    "        try:\n",
    "            result = prom.custom_query_range(\n",
    "                query=query,\n",
    "                start_time=start_time,\n",
    "                end_time=end_time,\n",
    "                step=step\n",
    "            )\n",
    "            # Assuming the result structure contains data in a format we expect\n",
    "            results[namespace] = result\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for namespace {namespace}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Define the namespace and the time range\n",
    "namespaces = ['default', 'kube-node-lease', 'kube-public', 'kube-system', 'kubernetes-dashboard', 'prometheus', 'ros']\n",
    "start_time = datetime(2024, 4, 20, 0, 0, 0)\n",
    "end_time = datetime(2024, 4, 20, 23, 59, 59)\n",
    "step = '60s'  # Step size\n",
    "metric_name = \"container_cpu_usage_seconds_total\"\n",
    "path = \"./dataset/\"\n",
    "\n",
    "# \n",
    "results = get_total_cpu_usage_for_namespace(metric_name=metric_name, namespaces=namespaces, aggregation_time='1m', start_time=start_time, end_time=end_time, step=step, per_pod=False)\n",
    "file_name = \"container_cpu_usage_seconds_total.json\"\n",
    "file_path = path+file_name\n",
    "with open(file_path, 'wt', encoding='utf-8') as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "\n",
    "# \n",
    "results = get_total_cpu_usage_for_namespace(metric_name=metric_name, namespaces=namespaces, aggregation_time='1m', start_time=start_time, end_time=end_time, step=step, per_pod=True)\n",
    "file_name = \"container_cpu_usage_seconds_total_pod.json\"\n",
    "file_path = path+file_name\n",
    "with open(file_path, 'wt', encoding='utf-8') as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "from datetime import datetime, timedelta  \n",
    "import pandas as pd \n",
    "import yaml\n",
    "import json\n",
    "\n",
    "# Initialize Prometheus Connection\n",
    "prom = PrometheusConnect(url =\"http://192.168.56.10:30090\", disable_ssl=True)\n",
    "\n",
    "\n",
    "def get_total_memory_usage_for_namespace(metric_name, namespaces, aggregation_time, start_time=None, end_time=None, step='60s', per_pod=False):\n",
    "\n",
    "    # Initialize the result dictionary\n",
    "    results = {}\n",
    "\n",
    "    # Loop through each namespace and fetch metrics\n",
    "    for namespace in namespaces:\n",
    "        label_part = f'namespace=\"{namespace}\"'\n",
    "        \n",
    "        # Modify the PromQL query to sum across all pods in the namespace\n",
    "        \n",
    "        if per_pod:\n",
    "            # query = f'sum(rate({metric_name}{{{label_part}}}[{aggregation_time}])) by (pod)'\n",
    "            query = f'sum({metric_name}{{{label_part}}}) by (pod)'\n",
    "        else:\n",
    "            # query = f'sum(rate({metric_name}{{{label_part}}}[{aggregation_time}]))'\n",
    "            query = f'sum({metric_name}{{{label_part}}})'\n",
    "\n",
    "\n",
    "        # Use default times if not specified\n",
    "        if not start_time:\n",
    "            start_time = datetime.datetime.now() - datetime.timedelta(hours=1)\n",
    "        if not end_time:\n",
    "            end_time = datetime.datetime.now()\n",
    "\n",
    "        # Convert datetime objects to ISO format\n",
    "        # start_time_iso = start_time.isoformat() + 'Z'\n",
    "        # end_time_iso = end_time.isoformat() + 'Z'\n",
    "\n",
    "        # Fetch metrics from Prometheus\n",
    "        try:\n",
    "            result = prom.custom_query_range(\n",
    "                query=query,\n",
    "                start_time=start_time,\n",
    "                end_time=end_time,\n",
    "                step=step\n",
    "            )\n",
    "            # Assuming the result structure contains data in a format we expect\n",
    "            results[namespace] = result\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for namespace {namespace}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Define the namespace and the time range\n",
    "namespaces = ['default', 'kube-node-lease', 'kube-public', 'kube-system', 'kubernetes-dashboard', 'prometheus', 'ros']\n",
    "start_time = datetime(2024, 4, 20, 0, 0, 0)\n",
    "end_time = datetime(2024, 4, 20, 23, 59, 59)\n",
    "step = '60s'  # Step size\n",
    "metric_name = \"container_memory_usage_bytes\"\n",
    "path = \"./dataset/\"\n",
    "\n",
    "\n",
    "\n",
    "results = get_total_memory_usage_for_namespace(metric_name=metric_name, namespaces=namespaces, aggregation_time='1m', start_time=start_time, end_time=end_time, step=step, per_pod=False)\n",
    "file_name = \"container_memory_usage_bytes.json\"\n",
    "file_path = path+file_name\n",
    "with open(file_path, 'wt', encoding='utf-8') as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "\n",
    "\n",
    "results = get_total_memory_usage_for_namespace(metric_name=metric_name, namespaces=namespaces, aggregation_time='1m', start_time=start_time, end_time=end_time, step=step, per_pod=True)\n",
    "file_name = \"container_memory_usage_bytes_pod.json\"\n",
    "file_path = path+file_name\n",
    "with open(file_path, 'wt', encoding='utf-8') as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "from datetime import datetime, timedelta  \n",
    "import pandas as pd \n",
    "import yaml\n",
    "import json\n",
    "\n",
    "# Initialize Prometheus Connection\n",
    "prom = PrometheusConnect(url =\"http://192.168.56.10:30090\", disable_ssl=True)\n",
    "\n",
    "\n",
    "def get_network_for_namespace(metric_name, namespaces, aggregation_time, start_time=None, end_time=None, step='60s', per_pod=False):\n",
    "\n",
    "    # Initialize the result dictionary\n",
    "    results = {}\n",
    "\n",
    "    # Loop through each namespace and fetch metrics\n",
    "    for namespace in namespaces:\n",
    "        label_part = f'namespace=\"{namespace}\"'\n",
    "        \n",
    "        # Modify the PromQL query to sum across all pods in the namespace\n",
    "        \n",
    "        if per_pod:\n",
    "            query = f'sum(rate({metric_name}{{{label_part}}}[{aggregation_time}])) by (pod)'\n",
    "            # query = f'sum({metric_name}{{{label_part}}}) by (pod)'\n",
    "        else:\n",
    "            query = f'sum(rate({metric_name}{{{label_part}}}[{aggregation_time}]))'\n",
    "            # query = f'sum({metric_name}{{{label_part}}})'\n",
    "\n",
    "\n",
    "        # Use default times if not specified\n",
    "        if not start_time:\n",
    "            start_time = datetime.datetime.now() - datetime.timedelta(hours=1)\n",
    "        if not end_time:\n",
    "            end_time = datetime.datetime.now()\n",
    "\n",
    "        # Convert datetime objects to ISO format\n",
    "        # start_time_iso = start_time.isoformat() + 'Z'\n",
    "        # end_time_iso = end_time.isoformat() + 'Z'\n",
    "\n",
    "        # Fetch metrics from Prometheus\n",
    "        try:\n",
    "            result = prom.custom_query_range(\n",
    "                query=query,\n",
    "                start_time=start_time,\n",
    "                end_time=end_time,\n",
    "                step=step\n",
    "            )\n",
    "            # Assuming the result structure contains data in a format we expect\n",
    "            results[namespace] = result\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for namespace {namespace}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# container_network_receive_bytes_total\n",
    "# Define the namespace and the time range\n",
    "namespaces = ['default', 'kube-node-lease', 'kube-public', 'kube-system', 'kubernetes-dashboard', 'prometheus', 'ros']\n",
    "start_time = datetime(2024, 4, 20, 0, 0, 0)\n",
    "end_time = datetime(2024, 4, 20, 23, 59, 59)\n",
    "step = '60s'  # Step size\n",
    "metric_name = \"container_network_receive_bytes_total\"\n",
    "path = \"./dataset/\"\n",
    "\n",
    "results = get_network_for_namespace(metric_name=metric_name, namespaces=namespaces, aggregation_time='1m', start_time=start_time, end_time=end_time, step=step, per_pod=False)\n",
    "\n",
    "file_name = \"container_network_receive_bytes_total.json\"\n",
    "file_path = path+file_name\n",
    "with open(file_path, 'wt', encoding='utf-8') as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "\n",
    "results = get_network_for_namespace(metric_name=metric_name, namespaces=namespaces, aggregation_time='1m', start_time=start_time, end_time=end_time, step=step, per_pod=True)\n",
    "\n",
    "file_name = \"container_network_receive_bytes_total_pod.json\"\n",
    "file_path = path+file_name\n",
    "with open(file_path, 'wt', encoding='utf-8') as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# container_network_transmit_bytes_total\n",
    "# Define the namespace and the time range\n",
    "namespaces = ['default', 'kube-node-lease', 'kube-public', 'kube-system', 'kubernetes-dashboard', 'prometheus', 'ros']\n",
    "start_time = datetime(2024, 4, 20, 0, 0, 0)\n",
    "end_time = datetime(2024, 4, 20, 23, 59, 59)\n",
    "step = '60s'  # Step size\n",
    "metric_name = \"container_network_transmit_bytes_total\"\n",
    "path = \"./dataset/\"\n",
    "\n",
    "results = get_network_for_namespace(metric_name=metric_name, namespaces=namespaces, aggregation_time='1m', start_time=start_time, end_time=end_time, step=step, per_pod=False)\n",
    "\n",
    "file_name = \"container_network_transmit_bytes_total.json\"\n",
    "file_path = path+file_name\n",
    "with open(file_path, 'wt', encoding='utf-8') as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "\n",
    "results = get_network_for_namespace(metric_name=metric_name, namespaces=namespaces, aggregation_time='1m', start_time=start_time, end_time=end_time, step=step, per_pod=True)\n",
    "\n",
    "file_name = \"container_network_transmit_bytes_total_pod.json\"\n",
    "file_path = path+file_name\n",
    "with open(file_path, 'wt', encoding='utf-8') as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disk Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "from datetime import datetime, timedelta  \n",
    "import pandas as pd \n",
    "import yaml\n",
    "import json\n",
    "\n",
    "# Initialize Prometheus Connection\n",
    "prom = PrometheusConnect(url =\"http://192.168.56.10:30090\", disable_ssl=True)\n",
    "\n",
    "\n",
    "def get_total_fs_usage_for_namespace(metric_name, namespaces, aggregation_time, start_time=None, end_time=None, step='60s', per_pod=False):\n",
    "\n",
    "    # Initialize the result dictionary\n",
    "    results = {}\n",
    "\n",
    "    # Loop through each namespace and fetch metrics\n",
    "    for namespace in namespaces:\n",
    "        label_part = f'namespace=\"{namespace}\"'\n",
    "        \n",
    "        # Modify the PromQL query to sum across all pods in the namespace\n",
    "        \n",
    "        if per_pod:\n",
    "            # query = f'sum(rate({metric_name}{{{label_part}}}[{aggregation_time}])) by (pod)'\n",
    "            query = f'sum({metric_name}{{{label_part}}}) by (pod)'\n",
    "        else:\n",
    "            # query = f'sum(rate({metric_name}{{{label_part}}}[{aggregation_time}]))'\n",
    "            query = f'sum({metric_name}{{{label_part}}})'\n",
    "\n",
    "\n",
    "        # Use default times if not specified\n",
    "        if not start_time:\n",
    "            start_time = datetime.datetime.now() - datetime.timedelta(hours=1)\n",
    "        if not end_time:\n",
    "            end_time = datetime.datetime.now()\n",
    "\n",
    "        # Convert datetime objects to ISO format\n",
    "        # start_time_iso = start_time.isoformat() + 'Z'\n",
    "        # end_time_iso = end_time.isoformat() + 'Z'\n",
    "\n",
    "        # Fetch metrics from Prometheus\n",
    "        try:\n",
    "            result = prom.custom_query_range(\n",
    "                query=query,\n",
    "                start_time=start_time,\n",
    "                end_time=end_time,\n",
    "                step=step\n",
    "            )\n",
    "            # Assuming the result structure contains data in a format we expect\n",
    "            results[namespace] = result\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for namespace {namespace}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Define the namespace and the time range\n",
    "namespaces = ['default', 'kube-node-lease', 'kube-public', 'kube-system', 'kubernetes-dashboard', 'prometheus', 'ros']\n",
    "start_time = datetime(2024, 4, 20, 0, 0, 0)\n",
    "end_time = datetime(2024, 4, 20, 23, 59, 59)\n",
    "step = '60s'  # Step size\n",
    "metric_name = \"container_fs_usage_bytes\"\n",
    "path = \"./dataset/\"\n",
    "\n",
    "\n",
    "\n",
    "results = get_total_fs_usage_for_namespace(metric_name=metric_name, namespaces=namespaces, aggregation_time='1m', start_time=start_time, end_time=end_time, step=step, per_pod=False)\n",
    "file_name = \"container_fs_usage_bytes.json\"\n",
    "file_path = path+file_name\n",
    "with open(file_path, 'wt', encoding='utf-8') as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = get_total_fs_usage_for_namespace(metric_name=metric_name, namespaces=namespaces, aggregation_time='1m', start_time=start_time, end_time=end_time, step=step, per_pod=True)\n",
    "file_name = \"container_fs_usage_bytes_pod.json\"\n",
    "file_path = path+file_name\n",
    "with open(file_path, 'wt', encoding='utf-8') as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f \n",
    "import json\n",
    "import pandas as pd\n",
    "import helper as hp\n",
    "import importlib as imp\n",
    "from pandas import json_normalize\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from matplotlib import dates as mdates\n",
    "imp.reload(hp)\n",
    "\n",
    "def transform_data(data):\n",
    "    # Convert the string representation of list in the 'values' column to actual lists\n",
    "    # Explode the 'values' column into multiple rows\n",
    "    exploded_data = data.explode('values')\n",
    "\n",
    "    # Split the 'values' column into 'timestamp' and 'value' columns\n",
    "    exploded_data[['timestamp', 'value']] = pd.DataFrame(exploded_data['values'].tolist(), index=exploded_data.index)\n",
    "\n",
    "\n",
    "    exploded_data['timestamp'] = pd.to_datetime(exploded_data['timestamp'], unit='s')\n",
    "    exploded_data.drop(columns=['values'], inplace=True, axis=0)\n",
    "    exploded_data.fillna(\"_\", inplace=True)\n",
    "    pivoted_data = exploded_data.pivot(index='timestamp', columns=[col for col in exploded_data.columns if col not in ['value', 'timestamp']], values='value')\n",
    "\n",
    "    if isinstance(pivoted_data, pd.Series):\n",
    "        pivoted_data = pd.DataFrame(pivoted_data, columns=['val'])\n",
    "        pivoted_data.reset_index(drop=False, inplace=True)\n",
    "        pivoted_data.set_index('timestamp', inplace=True) \n",
    "    \n",
    "    pivoted_data.sort_index(inplace=True) \n",
    "\n",
    "    # Remove the name of the column index\n",
    "    pivoted_data.columns.name = None    \n",
    "    pivoted_data.index.name = None\n",
    "\n",
    "    # Identify non-numeric columns\n",
    "    non_numeric_columns = pivoted_data.select_dtypes(exclude=['int', 'float']).columns\n",
    "\n",
    "    # Convert non-numeric columns to float\n",
    "    for col in non_numeric_columns:\n",
    "        pivoted_data[col] = pd.to_numeric(pivoted_data[col], errors='coerce')\n",
    "\n",
    "    return pivoted_data\n",
    "\n",
    "def json_transform(file_path, namespace):\n",
    "    with open(file_path, 'rt', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    df = json_normalize(data[namespace])\n",
    "    df = transform_data(json_normalize(data[namespace]))\n",
    "    return df\n",
    "\n",
    "def convert_bytes_to_readable(bytes, unit='MB'):\n",
    "    \"\"\"Convert bytes to higher units like MB, GB, etc.\"\"\"\n",
    "    factor = 1024\n",
    "    if unit == 'KB':\n",
    "        return bytes / factor\n",
    "    elif unit == 'MB':\n",
    "        return bytes / (factor ** 2)\n",
    "    elif unit == 'GB':\n",
    "        return bytes / (factor ** 3)\n",
    "    elif unit == 'TB':\n",
    "        return bytes / (factor ** 4)\n",
    "    else:\n",
    "        return bytes\n",
    "\n",
    "file_path = \"./dataset/container_memory_usage_bytes_pod.json\"\n",
    "# file_path = \"./dataset/container_cpu_usage_seconds_total_pod.json\"\n",
    "\n",
    "\n",
    "namespaces = [ 'kube-system', 'kubernetes-dashboard', 'prometheus', 'ros']\n",
    "\n",
    "df_ros = json_transform(file_path, namespace= 'ros')\n",
    "df_kube_system = json_transform(file_path, namespace='kube-system')\n",
    "df_kubernetes_dashboard = json_transform(file_path, namespace='kubernetes-dashboard')\n",
    "df_prometheus = json_transform(file_path, namespace='prometheus')\n",
    "\n",
    "df_ros = convert_bytes_to_readable(df_ros, unit='MB')\n",
    "df_kube_system = convert_bytes_to_readable(df_kube_system, unit='MB')\n",
    "df_kubernetes_dashboard = convert_bytes_to_readable(df_kubernetes_dashboard, unit='MB')\n",
    "df_prometheus = convert_bytes_to_readable(df_prometheus, unit='MB')\n",
    "\n",
    "dfs_memory = [df_ros, df_kube_system, df_kubernetes_dashboard, df_prometheus]\n",
    "titles = ['ROS2', 'kube-system', 'kubernetes-dashboard', 'Prometheus']\n",
    "\n",
    "start = \"2024-04-20 09:10:00\"\n",
    "end =   \"2024-04-20 15:00:00\"\n",
    "\n",
    "rows = len(dfs_memory)\n",
    "fig, ax = plt.subplots(rows, 1, figsize=(20, 5*rows), sharex=True)\n",
    "\n",
    "# Use ScalarFormatter to disable scientific notation\n",
    "formatter = ticker.ScalarFormatter(useOffset=False)\n",
    "formatter.set_scientific(False)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(rows):\n",
    "    for column in dfs_memory[i].columns:\n",
    "        ax[i].plot(dfs_memory[i].loc[start:end, column], label=column, linewidth=3)\n",
    "    ax[i].legend(title=titles[i], loc='upper left', bbox_to_anchor=(1,1)) # , markersize=5, linestyle='-'\n",
    "    ax[i].grid()\n",
    "    # Applying the formatter to the y-axes\n",
    "    ax[i].yaxis.set_major_formatter(formatter)\n",
    "    ax[i].set_ylabel('Memory Usage (MB)',fontsize=20)\n",
    "    ax[i].set_title(f\"Memory Usage for {titles[i]}\", fontsize=20, y=0.98, ha='center', color='black', backgroundcolor='lightgrey', weight='bold')\n",
    "    ax[i].set_ylim(bottom=0, top=1200)\n",
    "    ax[i].tick_params(axis='y', labelsize=15)  # Adjust labelsize as needed\n",
    "\n",
    "ax[-1].tick_params(axis='x', labelsize=15)  # Adjust labelsize as needed\n",
    "# Set the date format on the x-axis\n",
    "date_format = mdates.DateFormatter('%Y-%m-%d %H:%M')\n",
    "ax[-1].xaxis.set_major_formatter(date_format)\n",
    "plt.subplots_adjust(hspace=0.05)\n",
    "plt.savefig('memory_usage.pdf', format='pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f \n",
    "import json\n",
    "import pandas as pd\n",
    "import helper as hp\n",
    "import importlib as imp\n",
    "from pandas import json_normalize\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from matplotlib import dates as mdates\n",
    "imp.reload(hp)\n",
    "\n",
    "def transform_data(data):\n",
    "    # Convert the string representation of list in the 'values' column to actual lists\n",
    "    # Explode the 'values' column into multiple rows\n",
    "    exploded_data = data.explode('values')\n",
    "\n",
    "    # Split the 'values' column into 'timestamp' and 'value' columns\n",
    "    exploded_data[['timestamp', 'value']] = pd.DataFrame(exploded_data['values'].tolist(), index=exploded_data.index)\n",
    "\n",
    "\n",
    "    exploded_data['timestamp'] = pd.to_datetime(exploded_data['timestamp'], unit='s')\n",
    "    exploded_data.drop(columns=['values'], inplace=True, axis=0)\n",
    "    exploded_data.fillna(\"_\", inplace=True)\n",
    "    pivoted_data = exploded_data.pivot(index='timestamp', columns=[col for col in exploded_data.columns if col not in ['value', 'timestamp']], values='value')\n",
    "\n",
    "    if isinstance(pivoted_data, pd.Series):\n",
    "        pivoted_data = pd.DataFrame(pivoted_data, columns=['val'])\n",
    "        pivoted_data.reset_index(drop=False, inplace=True)\n",
    "        pivoted_data.set_index('timestamp', inplace=True) \n",
    "    \n",
    "    pivoted_data.sort_index(inplace=True) \n",
    "\n",
    "    # Remove the name of the column index\n",
    "    pivoted_data.columns.name = None    \n",
    "    pivoted_data.index.name = None\n",
    "\n",
    "    # Identify non-numeric columns\n",
    "    non_numeric_columns = pivoted_data.select_dtypes(exclude=['int', 'float']).columns\n",
    "\n",
    "    # Convert non-numeric columns to float\n",
    "    for col in non_numeric_columns:\n",
    "        pivoted_data[col] = pd.to_numeric(pivoted_data[col], errors='coerce')\n",
    "\n",
    "    return pivoted_data\n",
    "\n",
    "def json_transform(file_path, namespace):\n",
    "    with open(file_path, 'rt', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    df = json_normalize(data[namespace])\n",
    "    df = transform_data(json_normalize(data[namespace]))\n",
    "    return df\n",
    "\n",
    "def convert_bytes_to_readable(bytes, unit='MB'):\n",
    "    \"\"\"Convert bytes to higher units like MB, GB, etc.\"\"\"\n",
    "    factor = 1024\n",
    "    if unit == 'KB':\n",
    "        return bytes / factor\n",
    "    elif unit == 'MB':\n",
    "        return bytes / (factor ** 2)\n",
    "    elif unit == 'GB':\n",
    "        return bytes / (factor ** 3)\n",
    "    elif unit == 'TB':\n",
    "        return bytes / (factor ** 4)\n",
    "    else:\n",
    "        return bytes\n",
    "\n",
    "\n",
    "\n",
    "namespaces = ['ros', 'kube-system', 'kubernetes-dashboard', 'prometheus']\n",
    "titles = ['ROS2', 'kube-system', 'kubernetes-dashboard', 'Prometheus']\n",
    "\n",
    "dfs_memory = [json_transform(file_path=\"./dataset/container_memory_usage_bytes_pod.json\", namespace=namespace) for namespace in namespaces]\n",
    "dfs_cpu = [json_transform(file_path=\"./dataset/container_cpu_usage_seconds_total_pod.json\", namespace=namespace) for namespace in namespaces]\n",
    "\n",
    "\n",
    "dfs_memory = [convert_bytes_to_readable(df, unit='MB') for df in dfs_memory]\n",
    "# dfs_cpu = [convert_bytes_to_readable(df, unit='MB') for df in dfs_cpu]\n",
    "\n",
    "\n",
    "start = \"2024-04-20 09:10:00\"\n",
    "end =   \"2024-04-20 15:00:00\"\n",
    "\n",
    "rows = len(dfs_memory)\n",
    "fig, ax = plt.subplots(rows, 2, figsize=(30, 5*rows), sharex=True)\n",
    "\n",
    "#  Use ScalarFormatter to disable scientific notation\n",
    "formatter = ticker.ScalarFormatter(useOffset=False)\n",
    "formatter.set_scientific(False)\n",
    "\n",
    "for i in range(rows):\n",
    "    for column in dfs_cpu[i].columns:\n",
    "        ax[i,0].plot(dfs_cpu[i].loc[start:end, column], label=column, linewidth=3)\n",
    "    # ax[i,0].legend(title=titles[i], loc='upper left', bbox_to_anchor=(1,1)) # , markersize=5, linestyle='-'\n",
    "    ax[i,0].grid()\n",
    "    # Applying the formatter to the y-axes\n",
    "    ax[i,0].yaxis.set_major_formatter(formatter)\n",
    "    ax[i,0].set_ylabel('CPU Usage',fontsize=20)\n",
    "    ax[i,0].set_title(f\"CPU Usage for {titles[i]}\", fontsize=20, y=0.98, ha='center', color='black', backgroundcolor='lightgrey', weight='bold')\n",
    "    ax[i,0].set_ylim(bottom=0, top=0.26)\n",
    "    ax[i,0].tick_params(axis='y', labelsize=15)  # Adjust labelsize as needed\n",
    "\n",
    "ax[-1,0].tick_params(axis='x', labelsize=15)  # Adjust labelsize as needed\n",
    "# Set the date format on the x-axis\n",
    "date_format = mdates.DateFormatter('%Y-%m-%d %H:%M')\n",
    "ax[-1,0].xaxis.set_major_formatter(date_format)\n",
    "\n",
    "\n",
    "for i in range(rows):\n",
    "    for column in dfs_memory[i].columns:\n",
    "        ax[i,1].plot(dfs_memory[i].loc[start:end, column], label=column, linewidth=3)\n",
    "    ax[i,1].legend(title=titles[i], loc='upper left', bbox_to_anchor=(1,1)) # , markersize=5, linestyle='-'\n",
    "    ax[i,1].grid()\n",
    "    # Applying the formatter to the y-axes\n",
    "    ax[i,1].yaxis.set_major_formatter(formatter)\n",
    "    ax[i,1].set_ylabel('Memory Usage (MB)',fontsize=20)\n",
    "    ax[i,1].set_title(f\"Memory Usage for {titles[i]}\", fontsize=20, y=0.98, ha='center', color='black', backgroundcolor='lightgrey', weight='bold')\n",
    "    ax[i,1].set_ylim(bottom=0, top=1200)\n",
    "    ax[i,1].tick_params(axis='y', labelsize=15)  # Adjust labelsize as needed\n",
    "\n",
    "ax[-1,1].tick_params(axis='x', labelsize=15)  # Adjust labelsize as needed\n",
    "# Set the date format on the x-axis\n",
    "date_format = mdates.DateFormatter('%Y-%m-%d %H:%M')\n",
    "ax[-1,1].xaxis.set_major_formatter(date_format)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplots_adjust(hspace=0.05, wspace=0.1)\n",
    "plt.savefig('cpu_memory_usage.pdf', format='pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f \n",
    "import json\n",
    "import pandas as pd\n",
    "import helper as hp\n",
    "import importlib as imp\n",
    "from pandas import json_normalize\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from matplotlib import dates as mdates\n",
    "imp.reload(hp)\n",
    "\n",
    "def transform_data(data):\n",
    "    # Convert the string representation of list in the 'values' column to actual lists\n",
    "    # Explode the 'values' column into multiple rows\n",
    "    exploded_data = data.explode('values')\n",
    "\n",
    "    # Split the 'values' column into 'timestamp' and 'value' columns\n",
    "    exploded_data[['timestamp', 'value']] = pd.DataFrame(exploded_data['values'].tolist(), index=exploded_data.index)\n",
    "\n",
    "\n",
    "    exploded_data['timestamp'] = pd.to_datetime(exploded_data['timestamp'], unit='s')\n",
    "    exploded_data.drop(columns=['values'], inplace=True, axis=0)\n",
    "    exploded_data.fillna(\"_\", inplace=True)\n",
    "    pivoted_data = exploded_data.pivot(index='timestamp', columns=[col for col in exploded_data.columns if col not in ['value', 'timestamp']], values='value')\n",
    "\n",
    "    if isinstance(pivoted_data, pd.Series):\n",
    "        pivoted_data = pd.DataFrame(pivoted_data, columns=['val'])\n",
    "        pivoted_data.reset_index(drop=False, inplace=True)\n",
    "        pivoted_data.set_index('timestamp', inplace=True) \n",
    "    \n",
    "    pivoted_data.sort_index(inplace=True) \n",
    "\n",
    "    # Remove the name of the column index\n",
    "    pivoted_data.columns.name = None    \n",
    "    pivoted_data.index.name = None\n",
    "\n",
    "    # Identify non-numeric columns\n",
    "    non_numeric_columns = pivoted_data.select_dtypes(exclude=['int', 'float']).columns\n",
    "\n",
    "    # Convert non-numeric columns to float\n",
    "    for col in non_numeric_columns:\n",
    "        pivoted_data[col] = pd.to_numeric(pivoted_data[col], errors='coerce')\n",
    "\n",
    "    return pivoted_data\n",
    "\n",
    "def json_transform(file_path, namespace):\n",
    "    with open(file_path, 'rt', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    df = json_normalize(data[namespace])\n",
    "    df = transform_data(json_normalize(data[namespace]))\n",
    "    return df\n",
    "\n",
    "def convert_bytes_to_readable(bytes, unit='MB'):\n",
    "    \"\"\"Convert bytes to higher units like MB, GB, etc.\"\"\"\n",
    "    factor = 1024\n",
    "    if unit == 'KB':\n",
    "        return bytes / factor\n",
    "    elif unit == 'MB':\n",
    "        return bytes / (factor ** 2)\n",
    "    elif unit == 'GB':\n",
    "        return bytes / (factor ** 3)\n",
    "    elif unit == 'TB':\n",
    "        return bytes / (factor ** 4)\n",
    "    else:\n",
    "        return bytes\n",
    "\n",
    "\n",
    "\n",
    "namespaces = ['ros', 'kube-system', 'kubernetes-dashboard', 'prometheus']\n",
    "titles = ['ROS2', 'kube-system', 'kubernetes-dashboard', 'Prometheus']\n",
    "\n",
    "dfs_memory = [json_transform(file_path=\"./dataset/container_memory_usage_bytes_pod.json\", namespace=namespace) for namespace in namespaces]\n",
    "dfs_cpu = [json_transform(file_path=\"./dataset/container_cpu_usage_seconds_total_pod.json\", namespace=namespace) for namespace in namespaces]\n",
    "\n",
    "# Concatenating DataFrames along the columns\n",
    "dfs_net = [pd.concat([\n",
    "    json_transform(file_path=\"./dataset/container_network_receive_bytes_total_pod.json\", namespace=namespace).add_suffix(\"_receive\"),\n",
    "    -json_transform(file_path=\"./dataset/container_network_transmit_bytes_total_pod.json\", namespace=namespace).add_suffix(\"_transmit\")\n",
    "    ], axis=1) for namespace in namespaces]\n",
    "# dfs_rec = [json_transform(file_path=\"./dataset/container_network_receive_bytes_total_pod.json\", namespace=namespace) for namespace in namespaces]\n",
    "# dfs_trans = [json_transform(file_path=\"./dataset/container_network_transmit_bytes_total_pod.json\", namespace=namespace) for namespace in namespaces]\n",
    "\n",
    "\n",
    "\n",
    "dfs_memory = [convert_bytes_to_readable(df, unit='GB') for df in dfs_memory]\n",
    "dfs_net = [convert_bytes_to_readable(df, unit='KB') for df in dfs_net]\n",
    "dfs_cpu = [df*100 for df in dfs_cpu]\n",
    "\n",
    "\n",
    "start = \"2024-04-20 09:10:00\"\n",
    "end =   \"2024-04-20 15:00:00\"\n",
    "\n",
    "rows = len(dfs_memory)\n",
    "fig, ax = plt.subplots(rows, 3, figsize=(30, 5*rows), sharex=True)\n",
    "\n",
    "#  Use ScalarFormatter to disable scientific notation\n",
    "formatter = ticker.ScalarFormatter(useOffset=False)\n",
    "formatter.set_scientific(False)\n",
    "\n",
    "\n",
    "for i in range(rows):\n",
    "    for column in dfs_net[i].columns:\n",
    "        ax[i,0].plot(dfs_net[i].loc[start:end, column], label=column, linewidth=3)\n",
    "    # ax[i,0].legend(title=titles[i], loc='upper left', bbox_to_anchor=(1,1)) # , markersize=5, linestyle='-'\n",
    "    ax[i,0].grid()\n",
    "    # Applying the formatter to the y-axes\n",
    "    ax[i,0].yaxis.set_major_formatter(formatter)\n",
    "    ax[i,0].set_ylabel('Network (kB/s)',fontsize=20)\n",
    "    ax[i,0].set_title(f\"Network Receive/Transmit- {titles[i]}\", fontsize=20, y=0.98, ha='center', color='black', backgroundcolor='lightgrey', weight='bold')\n",
    "    # ax[i,0].set_ylim(bottom=0, top=1200)\n",
    "    ax[i,0].tick_params(axis='y', labelsize=15)  # Adjust labelsize as needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for column in dfs_memory[i].columns:\n",
    "        ax[i,1].plot(dfs_memory[i].loc[start:end, column], label=column, linewidth=3)\n",
    "    # ax[i,1].legend(title=titles[i], loc='upper left', bbox_to_anchor=(1,1)) # , markersize=5, linestyle='-'\n",
    "    ax[i,1].grid()\n",
    "    # Applying the formatter to the y-axes\n",
    "    ax[i,1].yaxis.set_major_formatter(formatter)\n",
    "    ax[i,1].set_ylabel('Memory Usage (GB)',fontsize=20)\n",
    "    ax[i,1].set_title(f\"Memory Usage - {titles[i]}\", fontsize=20, y=0.98, ha='center', color='black', backgroundcolor='lightgrey', weight='bold')\n",
    "    # ax[i,1].set_ylim(bottom=0, top=1200)\n",
    "    ax[i,1].tick_params(axis='y', labelsize=15)  # Adjust labelsize as needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for column in dfs_cpu[i].columns:\n",
    "        ax[i,2].plot(dfs_cpu[i].loc[start:end, column], label=column, linewidth=3)\n",
    "    ax[i,2].legend(title=titles[i], loc='upper left', bbox_to_anchor=(1,1)) # , markersize=5, linestyle='-'\n",
    "    ax[i,2].grid()\n",
    "    # Applying the formatter to the y-axes\n",
    "    ax[i,2].yaxis.set_major_formatter(formatter)\n",
    "    ax[i,2].set_ylabel('CPU Usage %',fontsize=20)\n",
    "    ax[i,2].set_title(f\"CPU Usage - {titles[i]}\", fontsize=20, y=0.98, ha='center', color='black', backgroundcolor='lightgrey', weight='bold')\n",
    "    # ax[i,2].set_ylim(bottom=0, top=0.26)\n",
    "    ax[i,2].tick_params(axis='y', labelsize=15)  # Adjust labelsize as needed\n",
    "\n",
    "\n",
    "# Set the date format on the x-axis\n",
    "date_format = mdates.DateFormatter('%Y-%m-%d %H:%M')\n",
    "\n",
    "for j in range(3):\n",
    "    ax[-1,j].tick_params(axis='x', labelsize=15, rotation=60)  # Adjust labelsize as needed\n",
    "    ax[-1,j].xaxis.set_major_formatter(date_format)\n",
    "    \n",
    "\n",
    "\n",
    "plt.subplots_adjust(hspace=0.05, wspace=0.2)\n",
    "plt.savefig('cpu_memory_net.pdf', format='pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
